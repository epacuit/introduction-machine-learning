{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65116126",
   "metadata": {},
   "source": [
    "(encoding-text)=\n",
    "# Encoding Text    \n",
    "\n",
    "Consider the following examples of movie reviews:\n",
    "\n",
    "\n",
    "Example 1 \n",
    "* Review 1A (negative) <br/>\n",
    "I thought the movie would be great, *but it is not*. The script is weak, the pacing is slow, and the ending feels pointless.\n",
    "* Review 1B (positive) <br/>\n",
    "I thought the movie would not be great, but it *is*. The script is strong, the pacing is brisk, and the ending feels meaningful.\n",
    "\n",
    "\n",
    "Example 2\n",
    "* Review 2A (negative)<br/>\n",
    "The acting is decent, but the plot is predictable and the jokes fall flat.\n",
    "* Review 2B (positive)<br/>\n",
    "The plot is predictable and the jokes fall flat, but the acting is decent.\n",
    "\n",
    "\n",
    "Example 3\n",
    "* Review 3A (negative)<br/>\n",
    "For the first ninety minutes I waited for something exciting to happen. Spoiler: it never does.\n",
    "* Review 3B (positive)<br/>\n",
    "For the first ninety minutes I waited for something exciting to happen, and when it finally does it is worth every second.\n",
    "\n",
    "\n",
    "A problem with a **bag of words** representation treats each review as the same multiset of words: \n",
    "$$\n",
    "vector=(acting:1, plot:1, predictable:1, but:1, \\ldots).\n",
    "$$ \n",
    "\n",
    "Because position is discarded, the classifier cannot learn that \"not\" negates what follows or that the clause after \"but\" usually carries the main sentiment.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e0aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01  0.01  0.02]\n",
      "  [ 0.04  0.03 -0.03]\n",
      "  [-0.05  0.03 -0.03]\n",
      "  [ 0.04  0.03 -0.01]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# 1. Toy vocabulary: 5 tokens\n",
    "#    0 = <PAD>, 1 = \"good\", 2 = \"bad\", 3 = \"not\", 4 = \"movie\"\n",
    "vocab_size   = 5\n",
    "embed_dim    = 3   # just three numbers per word\n",
    "sequence_len = 4\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embed_dim,\n",
    "                  input_length=sequence_len)\n",
    "\n",
    "# 2. Example review, integer‑encoded and padded:\n",
    "#    \"not good movie\"   →   [3, 1, 4, 0]\n",
    "sample = np.array([[3, 1, 4, 0]])        # shape (1, 4)\n",
    "\n",
    "dense_seq = embed(sample)                # shape (1, 4, 3)\n",
    "print(dense_seq.numpy().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33ca3a",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Unlike bag‑of‑words, use dense vectors that are trainable and will gradually move so that “good” and “bad” point in different directions, while “not” can flip the meaning when a sequential layer (CNN, RNN) reads the tokens in order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d82e17d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01 -0.01]\n",
      "  [-0.    0.03]\n",
      "  [ 0.03  0.04]\n",
      "  [ 0.01 -0.01]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# 1. Toy vocabulary: 5 tokens\n",
    "#    0 = <PAD>, 1 = \"good\", 2 = \"bad\", 3 = \"not\", 4 = \"movie\"\n",
    "vocab_size   = 6\n",
    "embed_dim    = 2   # just three numbers per word\n",
    "sequence_len = 4\n",
    "\n",
    "embed = Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embed_dim,\n",
    "                  input_length=sequence_len)\n",
    "\n",
    "# 2. Example review, integer‑encoded and padded:\n",
    "#    \"not good movie\"   →   [3, 1, 4, 0]\n",
    "sample = np.array([[3, 1, 5, 3]])        # shape (1, 4)\n",
    "\n",
    "dense_seq = embed(sample)                # shape (1, 4, 3)\n",
    "print(dense_seq.numpy().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61583075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d646c",
   "metadata": {},
   "source": [
    "## Using a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6410685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_20 (Embedding)    (None, 400, 128)          2560000   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 394, 64)           57408     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 131, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 125, 64)           28736     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Gl  (None, 64)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2646209 (10.09 MB)\n",
      "Trainable params: 2646209 (10.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Test accuracy: 0.848\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted probabilities: [0.    1.    0.937 0.484 1.    0.873 1.    0.    0.992 0.999]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Data loading and preprocessing\n",
    "# ------------------------------------------------------------------\n",
    "max_features = 20_000      # keep the 20 000 most frequent words\n",
    "maxlen        = 400        # cut / pad every review to 400 tokens\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test  = keras.preprocessing.sequence.pad_sequences(x_test,  maxlen=maxlen)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. CNN model\n",
    "# ------------------------------------------------------------------\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(max_features, 128, input_length=maxlen),\n",
    "\n",
    "    layers.Conv1D(64, 7, activation=\"relu\"),\n",
    "    layers.MaxPooling1D(3),\n",
    "\n",
    "    layers.Conv1D(64, 7, activation=\"relu\"),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Training\n",
    "# ------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=8,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Evaluation\n",
    "# ------------------------------------------------------------------\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Predictions (first ten test examples, rounded for readability)\n",
    "# ------------------------------------------------------------------\n",
    "preds = model.predict(x_test[:10]).round(3).squeeze()\n",
    "print(\"Predicted probabilities:\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fc5b4",
   "metadata": {},
   "source": [
    "A CNN is great at spotting local phrases like “not good,” but it quickly forgets where it found them; sentiment often hinges on relations far apart in the text, so we need a model that can keep track of information across the whole sequence.\n",
    "\n",
    "* A Conv1D with kernel size $k=7$ can only look at $7$ consecutive tokens at a time.\n",
    "* Stacking layers enlarges the receptive field only linearly (first layer sees 7 tokens, two layers see $\\approx 19$, etc.).\n",
    "* Important cues in a review are often dozens of tokens apart (\"I hoped it would be great...but it is not.\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8fae7",
   "metadata": {},
   "source": [
    "On the IMDB dataset, a  CNN typically reaches $~0.88$ accuracy, identical to the bag‑of‑words MLP, then plateaus. Extra filters or layers add parameters but do not solve the fundamental distance problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe8165",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
