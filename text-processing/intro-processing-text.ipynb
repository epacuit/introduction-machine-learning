{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab08999",
   "metadata": {},
   "source": [
    "(word-encodings=)\n",
    "# Encoding Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1b96c",
   "metadata": {},
   "source": [
    "## Download Movie Reviews Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  5506k      0  0:00:14  0:00:14 --:--:-- 6935k02  0:00:51 1523k\n"
     ]
    }
   ],
   "source": [
    "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ee5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbd81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\") \n",
    "val_dir = base_dir / \"val\" \n",
    "train_dir = base_dir / \"train\" \n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category) \n",
    "    files = os.listdir(train_dir / category) \n",
    "    random.Random(1337).shuffle(files) \n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "\n",
    "val_files = files[-num_val_samples:]\n",
    "for fname in val_files:\n",
    "    shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "169e1efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 files belonging to 2 classes.\n",
      "Found 2500 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size ) \n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size ) \n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14b24a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'While \"Santa Claus Conquers the Martians\" is usually cited as one of the worse films ever made, this Mexican-made film from 1959 is so bad it makes \"SCCM\" look like \"It\\'s a Wonderful Life.\" You have to wonder what the people who made this film were thinking; perhaps they meant it as a third-world allegory about capitalist greed and conspicuous consumption. Nah . . . They just weren\\'t very good. The same production company made an even more disturbing version of \"Little Red Riding Hood\" in which the wolf\\'s obsession with our heroine has unmistakable hints of pedophilia. (Perhaps this was the inspiration for \"Freeway.\") Back to \"Santa Claus\": instead of the North Pole, Jolly Old Saint Nicholas resides in a satellite in geosynchronous earth orbit (shades of \"MST3K\"); instead of elves his toys are made by children chosen from around the world; and he had sophisticated spy equipment to check just which kids are naughty and nice. The result is like an Orwellian outer space sweat shop. It\\'s enough to turn you off Christmas forever. This and other low-rent Mexican children\\'s\\' films were dubbed in English and widely distributed in the U.S. in the early 1960s; no wonder the sixties became such a turbulent period in American history. The baby boomers who were forced to endure these \"family\" films as children would be all too eager to turn revolutionary.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02209d0a",
   "metadata": {},
   "source": [
    "## Encoding Text - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "015d068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization( \n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ") \n",
    "text_only_train_ds = train_ds.map(lambda x, y: x) \n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "binary_1gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "binary_1gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aa6afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d43ee3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c592ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "678/704 [===========================>..] - ETA: 0s - loss: 0.3946 - accuracy: 0.8347\n",
      "Epoch 1: val_loss improved from inf to 0.34251, saving model to binary_1gram.weights.h5\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 0.3926 - accuracy: 0.8362 - val_loss: 0.3425 - val_accuracy: 0.8616\n",
      "Epoch 2/10\n",
      "678/704 [===========================>..] - ETA: 0s - loss: 0.2724 - accuracy: 0.8993\n",
      "Epoch 2: val_loss improved from 0.34251 to 0.31518, saving model to binary_1gram.weights.h5\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2727 - accuracy: 0.8995 - val_loss: 0.3152 - val_accuracy: 0.8752\n",
      "Epoch 3/10\n",
      "701/704 [============================>.] - ETA: 0s - loss: 0.2479 - accuracy: 0.9125\n",
      "Epoch 3: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2479 - accuracy: 0.9123 - val_loss: 0.3394 - val_accuracy: 0.8748\n",
      "Epoch 4/10\n",
      "681/704 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9184\n",
      "Epoch 4: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2392 - accuracy: 0.9186 - val_loss: 0.3386 - val_accuracy: 0.8736\n",
      "Epoch 5/10\n",
      "675/704 [===========================>..] - ETA: 0s - loss: 0.2339 - accuracy: 0.9240\n",
      "Epoch 5: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2342 - accuracy: 0.9238 - val_loss: 0.3817 - val_accuracy: 0.8508\n",
      "Epoch 6/10\n",
      "702/704 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9250\n",
      "Epoch 6: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2284 - accuracy: 0.9251 - val_loss: 0.3867 - val_accuracy: 0.8540\n",
      "Epoch 7/10\n",
      "689/704 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9281\n",
      "Epoch 7: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2276 - accuracy: 0.9279 - val_loss: 0.3691 - val_accuracy: 0.8580\n",
      "Epoch 8/10\n",
      "673/704 [===========================>..] - ETA: 0s - loss: 0.2321 - accuracy: 0.9272\n",
      "Epoch 8: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2318 - accuracy: 0.9276 - val_loss: 0.3486 - val_accuracy: 0.8752\n",
      "Epoch 9/10\n",
      "674/704 [===========================>..] - ETA: 0s - loss: 0.2208 - accuracy: 0.9325\n",
      "Epoch 9: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.2218 - accuracy: 0.9325 - val_loss: 0.3940 - val_accuracy: 0.8488\n",
      "Epoch 10/10\n",
      "693/704 [============================>.] - ETA: 0s - loss: 0.2232 - accuracy: 0.9332\n",
      "Epoch 10: val_loss did not improve from 0.31518\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2233 - accuracy: 0.9329 - val_loss: 0.4039 - val_accuracy: 0.8384\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2962 - accuracy: 0.8849\n",
      "Test acc: 0.885\n"
     ]
    }
   ],
   "source": [
    "model = get_model() \n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"binary_1gram.weights.h5\",   # any filename; .h5 is conventional\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,      # <—  key line\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.  Fit\n",
    "# --------------------------------------------------\n",
    "model.fit(\n",
    "    binary_1gram_train_ds.cache(),\n",
    "    validation_data=binary_1gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4.  Reload the best weights into an *identical* model\n",
    "# --------------------------------------------------\n",
    "best_model = keras.models.clone_model(model)      # architecture only\n",
    "best_model.compile(                              # ← compile it\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "best_model.load_weights(\"binary_1gram.weights.h5\")  # load the saved weights\n",
    "\n",
    "# Now it's ready for evaluation\n",
    "test_loss, test_acc = best_model.evaluate(binary_1gram_test_ds)\n",
    "print(f\"Test acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6c7a8",
   "metadata": {},
   "source": [
    "## Bigram Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fde02dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization( \n",
    "    ngrams=2, \n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97aaa8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "676/704 [===========================>..] - ETA: 0s - loss: 0.3785 - accuracy: 0.8450\n",
      "Epoch 1: val_loss improved from inf to 0.27960, saving model to binary_2gram.weights.h5\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 0.3765 - accuracy: 0.8460 - val_loss: 0.2796 - val_accuracy: 0.8964\n",
      "Epoch 2/10\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9152\n",
      "Epoch 2: val_loss improved from 0.27960 to 0.27515, saving model to binary_2gram.weights.h5\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2458 - accuracy: 0.9152 - val_loss: 0.2751 - val_accuracy: 0.8940\n",
      "Epoch 3/10\n",
      "687/704 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9312\n",
      "Epoch 3: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9310 - val_loss: 0.2782 - val_accuracy: 0.8984\n",
      "Epoch 4/10\n",
      "687/704 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9368\n",
      "Epoch 4: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9369 - val_loss: 0.3293 - val_accuracy: 0.8780\n",
      "Epoch 5/10\n",
      "686/704 [============================>.] - ETA: 0s - loss: 0.1919 - accuracy: 0.9427\n",
      "Epoch 5: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9428 - val_loss: 0.2993 - val_accuracy: 0.8912\n",
      "Epoch 6/10\n",
      "689/704 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9443\n",
      "Epoch 6: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9442 - val_loss: 0.3911 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "699/704 [============================>.] - ETA: 0s - loss: 0.1905 - accuracy: 0.9447\n",
      "Epoch 7: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.8704\n",
      "Epoch 8/10\n",
      "672/704 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.9464\n",
      "Epoch 8: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9460 - val_loss: 0.3417 - val_accuracy: 0.8692\n",
      "Epoch 9/10\n",
      "676/704 [===========================>..] - ETA: 0s - loss: 0.1831 - accuracy: 0.9481\n",
      "Epoch 9: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9478 - val_loss: 0.3404 - val_accuracy: 0.8648\n",
      "Epoch 10/10\n",
      "688/704 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9481\n",
      "Epoch 10: val_loss did not improve from 0.27515\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9481 - val_loss: 0.3599 - val_accuracy: 0.8524\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2694 - accuracy: 0.9012\n",
      "Test acc: 0.901\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds) \n",
    "binary_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "binary_2gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "binary_2gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n",
    "model = get_model() \n",
    "model.summary() \n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"binary_2gram.weights.h5\",   # any filename; .h5 is conventional\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,      # <—  key line\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.  Fit\n",
    "# --------------------------------------------------\n",
    "model.fit(\n",
    "    binary_2gram_train_ds.cache(),\n",
    "    validation_data=binary_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4.  Reload the best weights into an *identical* model\n",
    "# --------------------------------------------------\n",
    "best_model = keras.models.clone_model(model)      # architecture only\n",
    "best_model.compile(                              # ← compile it\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "best_model.load_weights(\"binary_2gram.weights.h5\")  # load the saved weights\n",
    "\n",
    "# Now it's ready for evaluation\n",
    "test_loss, test_acc = best_model.evaluate(binary_2gram_test_ds)\n",
    "print(f\"Test acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization( \n",
    "    ngrams=2,\n",
    "      max_tokens=20000,\n",
    "      output_mode=\"tf_idf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad27558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.4872 - accuracy: 0.7948\n",
      "Epoch 1: val_loss improved from inf to 0.25396, saving model to tfidf_2gram.weights.h5\n",
      "704/704 [==============================] - 2s 2ms/step - loss: 0.4872 - accuracy: 0.7948 - val_loss: 0.2540 - val_accuracy: 0.9292\n",
      "Epoch 2/10\n",
      "696/704 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8618\n",
      "Epoch 2: val_loss improved from 0.25396 to 0.23088, saving model to tfidf_2gram.weights.h5\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8623 - val_loss: 0.2309 - val_accuracy: 0.9356\n",
      "Epoch 3/10\n",
      "678/704 [===========================>..] - ETA: 0s - loss: 0.3021 - accuracy: 0.8740\n",
      "Epoch 3: val_loss improved from 0.23088 to 0.22145, saving model to tfidf_2gram.weights.h5\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.3038 - accuracy: 0.8731 - val_loss: 0.2214 - val_accuracy: 0.9580\n",
      "Epoch 4/10\n",
      "685/704 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.8860\n",
      "Epoch 4: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.8854 - val_loss: 0.2587 - val_accuracy: 0.8944\n",
      "Epoch 5/10\n",
      "681/704 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.8970\n",
      "Epoch 5: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2537 - accuracy: 0.8963 - val_loss: 0.3549 - val_accuracy: 0.8544\n",
      "Epoch 6/10\n",
      "677/704 [===========================>..] - ETA: 0s - loss: 0.2467 - accuracy: 0.9000\n",
      "Epoch 6: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2463 - accuracy: 0.8994 - val_loss: 0.3698 - val_accuracy: 0.8564\n",
      "Epoch 7/10\n",
      "688/704 [============================>.] - ETA: 0s - loss: 0.2355 - accuracy: 0.9038\n",
      "Epoch 7: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.9035 - val_loss: 0.3385 - val_accuracy: 0.8724\n",
      "Epoch 8/10\n",
      "680/704 [===========================>..] - ETA: 0s - loss: 0.2304 - accuracy: 0.9051\n",
      "Epoch 8: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2308 - accuracy: 0.9042 - val_loss: 0.4323 - val_accuracy: 0.8120\n",
      "Epoch 9/10\n",
      "687/704 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9062\n",
      "Epoch 9: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2294 - accuracy: 0.9060 - val_loss: 0.3392 - val_accuracy: 0.8636\n",
      "Epoch 10/10\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9046\n",
      "Epoch 10: val_loss did not improve from 0.22145\n",
      "704/704 [==============================] - 1s 2ms/step - loss: 0.2314 - accuracy: 0.9046 - val_loss: 0.3469 - val_accuracy: 0.8492\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3344 - accuracy: 0.8503\n",
      "Test acc: 0.850\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "tfidf_2gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "tfidf_2gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n",
    "\n",
    "model = get_model() \n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"tfidf_2gram.weights.h5\",   # any filename; .h5 is conventional\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,      # <—  key line\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3.  Fit\n",
    "# --------------------------------------------------\n",
    "model.fit(\n",
    "    tfidf_2gram_train_ds.cache(),\n",
    "    validation_data=tfidf_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4.  Reload the best weights into an *identical* model\n",
    "# --------------------------------------------------\n",
    "best_model = keras.models.clone_model(model)      # architecture only\n",
    "best_model.compile(                              # ← compile it\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "best_model.load_weights(\"tfidf_2gram.weights.h5\")  # load the saved weights\n",
    "\n",
    "# Now it's ready for evaluation\n",
    "test_loss, test_acc = best_model.evaluate(tfidf_2gram_test_ds)\n",
    "print(f\"Test acc: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d740153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600 \n",
    "max_tokens = 20000 \n",
    "text_vectorization = layers.TextVectorization( max_tokens=max_tokens,\n",
    "                                              output_mode=\"int\",\n",
    "                                              output_sequence_length=max_length,\n",
    "\n",
    ") \n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n",
    "int_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4) \n",
    "int_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c87a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.7878\n",
      "Epoch 1: val_loss improved from inf to 0.24510, saving model to embeddings_bidir_gru.weights.h5\n",
      "704/704 [==============================] - 148s 208ms/step - loss: 0.4665 - accuracy: 0.7878 - val_loss: 0.2451 - val_accuracy: 0.9112\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.8905\n",
      "Epoch 2: val_loss did not improve from 0.24510\n",
      "704/704 [==============================] - 147s 209ms/step - loss: 0.2978 - accuracy: 0.8905 - val_loss: 0.4216 - val_accuracy: 0.8416\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9172\n",
      "Epoch 3: val_loss improved from 0.24510 to 0.10876, saving model to embeddings_bidir_gru.weights.h5\n",
      "704/704 [==============================] - 146s 208ms/step - loss: 0.2379 - accuracy: 0.9172 - val_loss: 0.1088 - val_accuracy: 0.9572\n",
      "Epoch 4/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9292\n",
      "Epoch 4: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 0.2041 - accuracy: 0.9292 - val_loss: 0.4245 - val_accuracy: 0.8584\n",
      "Epoch 5/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9421\n",
      "Epoch 5: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 143s 202ms/step - loss: 0.1731 - accuracy: 0.9421 - val_loss: 0.8105 - val_accuracy: 0.7728\n",
      "Epoch 6/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9546\n",
      "Epoch 6: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 0.1411 - accuracy: 0.9546 - val_loss: 0.3019 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9620\n",
      "Epoch 7: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.1215 - accuracy: 0.9620 - val_loss: 0.4381 - val_accuracy: 0.8600\n",
      "Epoch 8/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9690\n",
      "Epoch 8: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 0.0990 - accuracy: 0.9690 - val_loss: 0.3779 - val_accuracy: 0.8884\n",
      "Epoch 9/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9712\n",
      "Epoch 9: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 141s 200ms/step - loss: 0.0913 - accuracy: 0.9712 - val_loss: 0.7051 - val_accuracy: 0.8316\n",
      "Epoch 10/10\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9766\n",
      "Epoch 10: val_loss did not improve from 0.10876\n",
      "704/704 [==============================] - 150s 213ms/step - loss: 0.0742 - accuracy: 0.9766 - val_loss: 0.4826 - val_accuracy: 0.8900\n",
      "782/782 [==============================] - 113s 144ms/step - loss: 0.5789 - accuracy: 0.8614\n",
      "Test acc: 0.861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\") \n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) \n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"embeddings_bidir_gru.weights.h5\",   # any filename; .h5 is conventional\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,      # <—  key line\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds, \n",
    "    validation_data=int_val_ds, \n",
    "    epochs=10, \n",
    "    callbacks=callbacks) \n",
    "\n",
    "\n",
    "best_model = keras.models.clone_model(model)      # architecture only\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "best_model.load_weights(\"embeddings_bidir_gru.weights.h5\")  # load the saved weights\n",
    "\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27af43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
