{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eec40e",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5060048d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, math, numpy as np\n",
    "from tensorflow.keras import mixed_precision\n",
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel, create_optimizer\n",
    "from pathlib import Path\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")   # GPU speed-up\n",
    "import tensorflow as tf, os\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cb5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 21:44:18.381807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2798/2798 [==============================] - 2408s 856ms/step - loss: 9.9474 - val_loss: 6.8138\n",
      "Epoch 2/3\n",
      "2798/2798 [==============================] - 2520s 901ms/step - loss: 5.3886 - val_loss: 3.8185\n",
      "Epoch 3/3\n",
      "2798/2798 [==============================] - 2501s 894ms/step - loss: 2.4547 - val_loss: 1.4297\n",
      "final validation perplexity: 4.18\n",
      "✓ saved to phpe400_finetuned\n"
     ]
    }
   ],
   "source": [
    "# ── 1.  tokenizer with course-specific special tokens ──────────────\n",
    "SPECIAL = [\"<|question|>\", \"<|answer|>\", \"<|statement|>\", \"<|end|>\"]\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token                     # GPT-2 needs explicit pad\n",
    "tok.add_special_tokens({\"additional_special_tokens\": SPECIAL})\n",
    "\n",
    "# ── 2.  read the corpus and pre-tokenise in one call ───────────────\n",
    "BLOCK = 512                                       # GPT-2 context span\n",
    "txt_path = \"data/clean_corpus.txt\"\n",
    "lines = Path(txt_path).read_text().splitlines()     # ≈ 1 line = 1 sample\n",
    "\n",
    "enc = tok(lines,\n",
    "          truncation=True,\n",
    "          max_length=BLOCK,\n",
    "          padding=\"max_length\",                     # left-padded to 1024\n",
    "          return_tensors=\"np\")                      # gives NumPy arrays\n",
    "\n",
    "input_ids      = enc[\"input_ids\"]                   # shape (N, 1024)\n",
    "attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "# ── 3.  wrap the arrays in tf.data  ────────────────────────────────\n",
    "def as_ds(arr):          # helper: slice a 2-D NumPy array\n",
    "    return tf.data.Dataset.from_tensor_slices(arr)\n",
    "\n",
    "ds_ids  = as_ds(input_ids)\n",
    "ds_mask = as_ds(attention_mask)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((ds_ids, ds_mask)).map(\n",
    "    lambda ids, mask: {\"input_ids\": ids,\n",
    "                       \"attention_mask\": mask,\n",
    "                       \"labels\": ids},      # causal-LM target = ids\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# ── 4.  train / valid split, shuffle, batch ────────────────────────\n",
    "SIZE   = tf.data.experimental.cardinality(dataset).numpy()\n",
    "split  = int(0.95 * SIZE)\n",
    "\n",
    "train_ds = (dataset.take(split)\n",
    "                     .shuffle(10_000)\n",
    "                     .batch(8, drop_remainder=True)\n",
    "                     .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "valid_ds = (dataset.skip(split)\n",
    "                     .batch(8, drop_remainder=True)\n",
    "                     .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# ── 5.  build & compile the model ──────────────────────────────────\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tok))             # **critical**\n",
    "\n",
    "EPOCHS  = 3\n",
    "STEPS   = tf.data.experimental.cardinality(train_ds).numpy() * EPOCHS\n",
    "WARMUP  = int(0.1 * STEPS)\n",
    "\n",
    "opt, lr_schedule = create_optimizer(\n",
    "        init_lr=5e-5,\n",
    "        num_train_steps=STEPS,\n",
    "        num_warmup_steps=WARMUP,\n",
    "        weight_decay_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=opt)                        # HF supplies loss\n",
    "\n",
    "# ── 6.  train ──────────────────────────────────────────────────────\n",
    "hist = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=EPOCHS,\n",
    "        # callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        #                monitor=\"val_loss\",\n",
    "        #                patience=2,\n",
    "        #                restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"final validation perplexity:\",\n",
    "      round(math.exp(hist.history[\"val_loss\"][-1]), 2))\n",
    "\n",
    "# ── 7.  save checkpoint ────────────────────────────────────────────\n",
    "SAVE_DIR = \"phpe400_finetuned\"\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tok.save_pretrained(SAVE_DIR)\n",
    "print(\"✓ saved to\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ef0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at phpe400_finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "tok   = GPT2TokenizerFast.from_pretrained(\"phpe400_finetuned\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"phpe400_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6454760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is a rational preference?  ��� in���� the�U����anceteness���et�\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<|question|> What is a rational preference? <|answer|> \"\n",
    "inputs  = tok(prompt, return_tensors=\"tf\")\n",
    "\n",
    "eos_id  = tok.convert_tokens_to_ids(\"<|end|>\")      # the end-marker you added\n",
    "gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=240,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=eos_id,\n",
    ")\n",
    "\n",
    "print(tok.decode(gen_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399db2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
