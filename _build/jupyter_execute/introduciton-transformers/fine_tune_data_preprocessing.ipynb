{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eec40e",
   "metadata": {},
   "source": [
    "(fine-tuning-preprocessing)=\n",
    "# Fine-Tuning: Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b1e65",
   "metadata": {},
   "source": [
    "## Convert Slides (LaTeX) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8708b3fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'extracted/raw_slides.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m     txt \u001b[38;5;241m=\u001b[39m pypandoc\u001b[38;5;241m.\u001b[39mconvert_file(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mstr\u001b[39m(tex_path),\n\u001b[1;32m     10\u001b[0m         to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         extra_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--wrap=none\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--quiet\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, txt)  \u001b[38;5;66;03m# collapse blank lines\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tex \u001b[38;5;129;01min\u001b[39;00m Path(input_dir)\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.tex\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1012\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'extracted/raw_slides.txt'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import pypandoc, re\n",
    "\n",
    "input_dir = 'phpe400_corpus/slides'\n",
    "output_file = 'extracted/raw_slides.txt'\n",
    "\n",
    "def tex_to_plain(tex_path):\n",
    "    txt = pypandoc.convert_file(\n",
    "        str(tex_path),\n",
    "        to=\"plain\",\n",
    "        format=\"latex\",\n",
    "        extra_args=[\"--wrap=none\", \"--quiet\"])\n",
    "    return re.sub(r'\\n\\s*\\n+', '\\n', txt)  # collapse blank lines\n",
    "\n",
    "with Path(output_file).open(\"a\", encoding=\"utf-8\") as out:\n",
    "    for tex in Path(input_dir).rglob(\"*.tex\"):\n",
    "        print(f\"Processing {tex}\")\n",
    "        out.write(tex_to_plain(tex) + \"\\n\")\n",
    "\n",
    "print(f\"\\nOutput written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb119a",
   "metadata": {},
   "source": [
    "## Convert Syllabus (PDF) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb62309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phpe400_corpus/syllabus/syl-methods-ppe-v4.pdf\n",
      "\n",
      "Output written to extracted/raw_syllabus.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "import re, textwrap\n",
    "\n",
    "SRC  = \"phpe400_corpus/syllabus/syl-methods-ppe-v4.pdf\"    # the file you just uploaded\n",
    "DEST = \"extracted/raw_syllabus.txt\"            # append or create\n",
    "\n",
    "def pdf_to_plain(path):\n",
    "    reader = PdfReader(path)\n",
    "    lines  = []\n",
    "    for page in reader.pages:\n",
    "        txt = page.extract_text() or \"\"\n",
    "        txt = txt.replace(\"\\u200b\", \"\")          # zero-width\n",
    "        # strip page headers/footers like “5 / 8”\n",
    "        txt = re.sub(r'\\b\\d+\\s*/\\s*\\d+\\s*$', '', txt, flags=re.M)\n",
    "        lines.extend(txt.splitlines())\n",
    "    # collapse blocks of ≥2 blank lines to a single blank\n",
    "    cleaned = \"\\n\".join(line.rstrip() for line in lines)\n",
    "    cleaned = re.sub(r'\\n\\s*\\n+', '\\n', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "print(f\"Processing {SRC}\")\n",
    "plain = pdf_to_plain(SRC)\n",
    "\n",
    "# stream into your master corpus file\n",
    "with Path(DEST).open(\"a\", encoding=\"utf-8\") as out:\n",
    "    for para in plain.split(\"\\n\"):\n",
    "        if para.strip():\n",
    "            out.write(para.strip() + \"\\n\")\n",
    "print(f\"\\nOutput written to {DEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74dae93",
   "metadata": {},
   "source": [
    "## Convert Review Sheets (LaTeX) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7282a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phpe400_corpus/review-sheets/exam1-review-answers.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phpe400_corpus/review-sheets/final-exam-review.tex\n",
      "Processing phpe400_corpus/review-sheets/exam1-review.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phpe400_corpus/review-sheets/final-exam-review-answers.tex\n",
      "\n",
      "Output written to extracted/raw_review-sheets.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import pypandoc, re\n",
    "\n",
    "input_dir = 'phpe400_corpus/review-sheets'\n",
    "output_file = 'extracted/raw_review-sheets.txt'\n",
    "\n",
    "def tex_to_plain(tex_path):\n",
    "    txt = pypandoc.convert_file(\n",
    "        str(tex_path),\n",
    "        to=\"plain\",\n",
    "        format=\"latex\",\n",
    "        extra_args=[\"--wrap=none\", \"--quiet\"])\n",
    "    return re.sub(r'\\n\\s*\\n+', '\\n', txt)  # collapse blank lines\n",
    "\n",
    "with Path(output_file).open(\"a\", encoding=\"utf-8\") as out:\n",
    "    for tex in Path(input_dir).rglob(\"*.tex\"):\n",
    "        print(f\"Processing {tex}\")\n",
    "        out.write(tex_to_plain(tex) + \"\\n\")\n",
    "\n",
    "print(f\"\\nOutput written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff006d",
   "metadata": {},
   "source": [
    "## Convert Chapters (PDF) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac1aed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing econ-analysis-moral-phil-public-policy-ch4.pdf\n",
      "Processing econ-analysis-moral-philosophy-public-policy-ch13.pdf\n",
      "Processing econ-analysis-moral-philosophy-public-policy-ch14.pdf\n",
      "\n",
      "Output written to raw_text.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "\n",
    "input_dir = 'phpe400_corpus/text'  # folder with your 3 PDFs\n",
    "output_file = 'extracted/raw_text.txt'\n",
    "\n",
    "# --------------------------- 1. header & noise patterns --------------------------\n",
    "HEADER_RE = re.compile(\n",
    "    r'^[ \\t]*(?:Social Choice Theory|Game Theory|Rationality and Utility Theory)'\n",
    "    r'(?:[ \\t]+\\d+)?[ \\t]*$',                     # optional page number\n",
    "    flags=re.M\n",
    ")\n",
    "# Lone page numbers like “55”\n",
    "LONE_PAGE_RE   = re.compile(r'^[ \\t]*\\d{1,3}[ \\t]*$', flags=re.M)\n",
    "# All-caps chapter banners such as “FOUR” or “PART II”\n",
    "CAPS_BANNER_RE = re.compile(r'^[ \\t]*[A-Z]{2,}[ \\t]*$', flags=re.M)\n",
    "# Numbered section headings “4.1 …” or “2.3.5 …”\n",
    "SECNUM_RE      = re.compile(r'^\\d+(?:\\.\\d+)+\\s+.*$', flags=re.M)\n",
    "\n",
    "# --------------------------- 2. tidy helper --------------------------------------\n",
    "def tidy(text: str) -> str:\n",
    "    text = HEADER_RE.sub('', text)\n",
    "    text = LONE_PAGE_RE.sub('', text)\n",
    "    text = CAPS_BANNER_RE.sub('', text)\n",
    "    text = SECNUM_RE.sub('', text)\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)',  r'\\1\\2', text)   # hard hyphen breaks\n",
    "    text = re.sub(r'(\\w+)\\u00ad(\\w+)', r'\\1\\2', text) # soft hyphen\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n', text)            # collapse blank lines\n",
    "    return text.strip()\n",
    "\n",
    "# --------------------------- 3. PDF → plain-text ---------------------------------\n",
    "def pdf_to_plain(path: Path) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    pages  = []\n",
    "    for pg in reader.pages:\n",
    "        raw = pg.extract_text() or \"\"\n",
    "        raw = raw.replace('\\u200b', '')               # zero-width chars\n",
    "        raw = re.sub(r'\\b\\d+\\s*/\\s*\\d+\\s*$', '', raw, flags=re.M)  # “5 / 15” style\n",
    "        pages.append(tidy(raw))\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "# --------------------------- 4. batch over all PDFs ------------------------------\n",
    "PDF_DIR = Path(input_dir)   # folder with your 3 PDFs\n",
    "DEST    = Path(output_file)       # master output file\n",
    "\n",
    "with DEST.open(\"a\", encoding=\"utf-8\") as out:\n",
    "    for pdf in sorted(PDF_DIR.glob(\"*.pdf\")):\n",
    "        print(f\"Processing {pdf.name}\")\n",
    "        for para in pdf_to_plain(pdf).split(\"\\n\"):\n",
    "            if para.strip():\n",
    "                out.write(para + \"\\n\") \n",
    "\n",
    "print(\"\\nOutput written to\", DEST.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9ae35",
   "metadata": {},
   "source": [
    "## Convert Piazza Comments/Answers (JSON) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e62b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 56 Q/A pairs to extracted/raw_piazza_qa.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, html\n",
    "import re\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "PIAZZA = Path(\"phpe400_corpus/piazza/class_content_flat.json\")     # uploaded file\n",
    "OUT    = Path(\"extracted/raw_piazza_qa.txt\")\n",
    "\n",
    "def clean(html_snippet: str) -> str:\n",
    "    \"\"\"Remove <tags>, decode entities and collapse whitespace.\"\"\"\n",
    "    # 1. HTML → plain text\n",
    "    text = BeautifulSoup(html_snippet, \"html.parser\").get_text(\" \", strip=True)\n",
    "    # 2. Unescape &nbsp; etc.\n",
    "    text = html.unescape(text)\n",
    "    # 3. Tighten spaces/newlines\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "with PIAZZA.open() as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "# Index every post by its Piazza ID so we can match answers to questions\n",
    "by_id = {p[\"id\"]: p for p in posts}\n",
    "\n",
    "qapairs = []\n",
    "\n",
    "for p in posts:\n",
    "    if p[\"type\"] == \"question\":\n",
    "        qtxt = f\"Q: {clean(p['subject'])}\\n{clean(p['content'])}\"\n",
    "        # grab instructor answer (type == 'i_answer') in same thread\n",
    "        ans = next((by_id[c] for c in by_id            # walk once over dict\n",
    "                    if by_id[c].get(\"parent_id\") == p[\"id\"]\n",
    "                    and by_id[c][\"type\"] == \"i_answer\"),\n",
    "                   None)\n",
    "        if ans:\n",
    "            atxt = clean(ans[\"content\"])\n",
    "            qapairs.append(f\"{qtxt}\\nA: {atxt}\")\n",
    "\n",
    "OUT.write_text(\"\\n\\n\".join(qapairs), encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(qapairs)} Q/A pairs to {OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80956cf6",
   "metadata": {},
   "source": [
    "## Convert Online Notes (HTML) to Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55721bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing evaluative-voting.html\n",
      "Processing grading-vs-ranking.html\n",
      "Processing interpersonal-comparison-utilities.html\n",
      "Processing objections-utilitarianism.html\n",
      "Processing social-welfare-functionals.html\n",
      "Processing decision-problems.html\n",
      "Processing newcomb.html\n",
      "Processing rational-decisions.html\n",
      "Processing allais.html\n",
      "Processing ellsberg.html\n",
      "Processing evaluating-axioms.html\n",
      "Processing expected-utility.html\n",
      "Processing independence.html\n",
      "Processing functions.html\n",
      "Processing lotteries.html\n",
      "Processing preferences-over-lotteries.html\n",
      "Processing utility-functions.html\n",
      "Processing index.html\n",
      "Processing completeness.html\n",
      "Processing relations.html\n",
      "Processing sets.html\n",
      "Processing preference-and-choice.html\n",
      "Processing preference-relations.html\n",
      "Processing rational-preferences.html\n",
      "Processing transitivity.html\n",
      "Processing references.html\n",
      "Processing arrows-theorem.html\n",
      "Processing beyond-two-alternatives.html\n",
      "Processing condorcet-jury-theorem.html\n",
      "Processing justifying-majority-rule.html\n",
      "Processing mays-theorem.html\n",
      "Processing social-welfare-functions.html\n",
      "Processing voting-axioms.html\n",
      "Processing comparing-voting-methods.html\n",
      "Processing condorcet-consistent-methods.html\n",
      "Processing condorcet-paradox.html\n",
      "Processing elections.html\n",
      "Processing iterative-methods.html\n",
      "Processing majority-preference.html\n",
      "Processing scoring-rules.html\n",
      "Processing voting-methods.html\n",
      "\n",
      " Output 41 html files to raw_html_notes.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text, html, re\n",
    "\n",
    "# === 1. one-time imports & (auto-)installs =========================\n",
    "import sys, subprocess, re, html, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "for pkg in (\"beautifulsoup4\", \"html2text\", \"lxml\"):\n",
    "    if not shutil.which(\"pip\") or subprocess.call(\n",
    "          [sys.executable, \"-m\", \"pip\", \"show\", pkg],\n",
    "          stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "# === 2. configure the directory to scan ============================\n",
    "HTML_DIR = Path(\"phpe400_corpus/notes\")    # <── change if your html lives elsewhere\n",
    "DEST     = Path(\"extracted/raw_html_notes.txt\")\n",
    "\n",
    "# === 3. regex helpers ==============================================\n",
    "SPAN_MATH  = re.compile(r'<span[^>]*class=\"math[^\"]*\"(?:[^>]*>)(.*?)</span>', re.S)\n",
    "SCR_MATH   = re.compile(r'<script[^>]*type=\"math/tex[^\"]*\"(?:[^>]*>)(.*?)</script>', re.S)\n",
    "IMG_TAG    = re.compile(r'<img[^>]*>', re.S)\n",
    "LINE_JUNK  = re.compile(r'^(?:__+|\\s*[*\\-]\\s*$|\\s*\\d+\\.\\s+\\S.*)$')\n",
    "SIDEBAR_KW = (\"sidebar\", \"toc\", \"nav-page\", \"menu\")\n",
    "\n",
    "# === 4. utility functions ==========================================\n",
    "def strip_outer(expr: str) -> str:\n",
    "    expr = expr.strip()\n",
    "    if expr.startswith(r'\\(') and expr.endswith(r'\\)'): expr = expr[2:-2].strip()\n",
    "    elif expr.startswith(r'\\[') and expr.endswith(r'\\]'): expr = expr[2:-2].strip()\n",
    "    return re.sub(r'\\\\\\\\([{}])', r'\\\\\\1', expr)  # \\\\{ -> \\{   \\\\} -> \\}\n",
    "\n",
    "def looks_sidebar(tag) -> bool:\n",
    "    if tag.name in (\"nav\", \"aside\"): return True\n",
    "    blob = \" \".join([tag.get(\"id\", \"\"), *tag.get(\"class\", [])]).lower()\n",
    "    return any(k in blob for k in SIDEBAR_KW)\n",
    "\n",
    "h2t = html2text.HTML2Text(); h2t.ignore_links = True; h2t.body_width = 0\n",
    "\n",
    "def html_to_plain(html_file: Path) -> str:\n",
    "    soup  = BeautifulSoup(html_file.read_text(errors=\"ignore\"), \"lxml\")\n",
    "    main  = soup.find(\"main\", id=\"quarto-content\") or soup.find(\"main\") or soup.body or soup\n",
    "    soup  = BeautifulSoup(str(main), \"lxml\")            # clone so .decompose() is safe\n",
    "    for tag in soup.find_all(looks_sidebar): tag.decompose()\n",
    "\n",
    "    raw = IMG_TAG.sub('', str(soup))\n",
    "    raw = SPAN_MATH.sub(lambda m: f\"${strip_outer(m.group(1))}$\", raw)\n",
    "    raw = SCR_MATH.sub( lambda m: f\"${strip_outer(m.group(1))}$\", raw)\n",
    "\n",
    "    text = h2t.handle(raw)\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'\\\\\\\\([{}])', r'\\\\\\1', text)          # collapse any \\\\{ left\n",
    "\n",
    "    lines = [ln.strip() for ln in text.splitlines()\n",
    "             if ln.strip() and not LINE_JUNK.match(ln)]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# === 5. process every html file ====================================\n",
    "DEST.write_text(\"\", encoding=\"utf-8\")   # overwrite\n",
    "count = 0\n",
    "for html_f in sorted(HTML_DIR.rglob(\"*.html\")):\n",
    "    print(f\"Processing {html_f.name}\")\n",
    "    cleaned = html_to_plain(html_f)\n",
    "    DEST.write_text(DEST.read_text() + cleaned + \"\\n\", encoding=\"utf-8\")\n",
    "    count += 1\n",
    "print(f\"\\n Output {count} html files to {DEST.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546a2fd",
   "metadata": {},
   "source": [
    "## Combine All Text Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baebf4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ adding raw_slides.txt\n",
      "✓ adding raw_syllabus.txt\n",
      "✓ adding raw_review-sheets.txt\n",
      "✓ adding raw_text.txt\n",
      "✓ adding raw_piazza_qa.txt\n",
      "✓ adding raw_html_notes.txt\n",
      "\n",
      "wrote combined corpus → extracted/raw_phpe400_corpus.txt  (1328.9 KB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_text_files = [\n",
    "    \"raw_slides.txt\",\n",
    "    \"raw_syllabus.txt\",\n",
    "    \"raw_review-sheets.txt\",\n",
    "    \"raw_text.txt\",\n",
    "    \"raw_piazza_qa.txt\",\n",
    "    \"raw_html_notes.txt\"\n",
    "]\n",
    "\n",
    "MASTER = Path(\"extracted/raw_phpe400_corpus.txt\")\n",
    "MASTER.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with MASTER.open(\"w\", encoding=\"utf-8\") as master:\n",
    "    for raw in raw_text_files:\n",
    "        p = Path(raw)\n",
    "        if not p.exists():\n",
    "            print(f\"⚠  {raw} not found — skipping\")\n",
    "            continue\n",
    "        print(f\"✓ adding {raw}\")\n",
    "        for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
    "            if line.strip():               # skip truly empty lines\n",
    "                master.write(line.strip() + \"\\n\")\n",
    "        master.write(\"<|eod|>\\n\")          # <-- boundary token\n",
    "\n",
    "print(f\"\\nwrote combined corpus → {MASTER}  ({MASTER.stat().st_size/1024:.1f} KB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05cb4221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ processed raw_html_notes.txt\n",
      "✓ processed raw_syllabus.txt\n",
      "✓ processed raw_text.txt\n",
      "✓ processed raw_phpe400_corpus.txt\n",
      "✓ processed raw_piazza_qa.txt\n",
      "✓ processed raw_slides.txt\n",
      "✓ processed raw_review-sheets.txt\n",
      "✓ wrote data/clean_corpus.txt  —  580 tagged blocks\n"
     ]
    }
   ],
   "source": [
    "import re, textwrap, random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 0. directories\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "EXTRACT_DIR = Path(\"extracted\")           # all *.txt live here\n",
    "OUT_DIR     = Path(\"data\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1. helper:  Q:  ...  A:  ...  →  <|question|> ... <|answer|> ... <|end|>\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def convert_QA_blocks(text:str) -> str:\n",
    "    pat = re.compile(\n",
    "        r\"^Q:\\s*(?P<q>.*?)\\nA:\\s*(?P<a>.*?)(?=^\\s*\\n|\\Z)\",\n",
    "        flags=re.S | re.M)\n",
    "    def repl(m):\n",
    "        q = textwrap.dedent(m.group(\"q\")).strip()\n",
    "        a = textwrap.dedent(m.group(\"a\")).strip()\n",
    "        return f\"<|question|>\\n{q}\\n<|answer|>\\n{a}\\n<|end|>\"\n",
    "    return pat.sub(repl, text)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. helper: full cleaner you already tested\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def clean_text_block(text:str) -> str:\n",
    "    text = re.sub(r\"^[\\u2022\\-\\*-]\\s*\", \"\", text, flags=re.M)    # bullets\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)                       # blank lines\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)                       # 2+ spaces\n",
    "    H_RULE = re.compile(r\"^[\\s\\--—_=]{3,}$\", flags=re.M)         # rules\n",
    "    text   = re.sub(H_RULE, \"\", text)\n",
    "    seen, out = set(), []\n",
    "    for line in text.splitlines():                               # dedupe titles\n",
    "        if re.fullmatch(r\"[A-Z][A-Z ]{2,40}\", line.strip()):\n",
    "            if line in seen: continue\n",
    "            seen.add(line)\n",
    "        out.append(line)\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. helper: split long prose into <|statement|> blocks\n",
    "#            (but skip lines that are already tagged)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def paragraph_blocks(text:str, max_chars=3500):\n",
    "    if text.startswith(\"<|question|>\"):          # already tagged, keep as-is\n",
    "        return [text]\n",
    "    buf, out = [], []\n",
    "    for line in text.splitlines():\n",
    "        buf.append(line)\n",
    "        if len(\" \".join(buf)) > max_chars:\n",
    "            out.append(\"\\n\".join(buf)); buf=[]\n",
    "    if buf: out.append(\"\\n\".join(buf))\n",
    "    return [f\"<|statement|>\\n{b.strip()}\\n<|end|>\" for b in out]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4. load every extracted *.txt  (except qa_pairs.txt)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "plain_blocks = []\n",
    "for txt_file in EXTRACT_DIR.glob(\"*.txt\"):\n",
    "    if txt_file.name == \"qa_pairs.txt\":\n",
    "        continue\n",
    "    raw = txt_file.read_text()\n",
    "    raw = convert_QA_blocks(raw)       # convert embedded Q/A\n",
    "    clean = clean_text_block(raw)      # your cleaner\n",
    "    plain_blocks.extend(paragraph_blocks(clean))\n",
    "    print(\"✓ processed\", txt_file.name)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5. load hand-crafted Q-A file, dedupe, oversample\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "qa_raw    = Path(EXTRACT_DIR / \"qa_pairs.txt\").read_text()\n",
    "def dedupe_qas(raw):\n",
    "    seen, out = set(), []\n",
    "    for m in re.finditer(r\"<\\|question\\|>.*?<\\|end\\|>\", raw, re.S):\n",
    "        blk = textwrap.dedent(m.group(0)).strip()\n",
    "        if blk not in seen:\n",
    "            out.append(blk); seen.add(blk)\n",
    "    return out\n",
    "\n",
    "qa_blocks  = dedupe_qas(qa_raw)\n",
    "qa_blocks *= 2          # oversample factor (2 = duplicate once)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6. shuffle & write corpus\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "all_blocks = qa_blocks + plain_blocks\n",
    "random.shuffle(all_blocks)\n",
    "\n",
    "out_file = OUT_DIR / \"clean_corpus.txt\"\n",
    "out_file.write_text(\"\\n\\n\".join(all_blocks), encoding=\"utf-8\")\n",
    "print(f\"✓ wrote {out_file}  —  {len(all_blocks)} tagged blocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8728acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552851 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 552,851\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "corpus = Path(\"data/clean_corpus.txt\").read_text(encoding=\"utf-8\")\n",
    "n_tokens = len(tokenizer(corpus).input_ids)\n",
    "\n",
    "print(f\"Total tokens: {n_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64011946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}