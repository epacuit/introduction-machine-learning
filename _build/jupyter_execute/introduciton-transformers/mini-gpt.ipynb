{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5840e8e",
   "metadata": {},
   "source": [
    "(mini-gpt)=\n",
    "# Mini GPT\n",
    "\n",
    "This notebook will create a mini GPT using the IMDB dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522dadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html, random, numpy as np, tensorflow as tf, keras\n",
    "import keras_nlp as knlp\n",
    "from pathlib import Path\n",
    "#tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a743816",
   "metadata": {},
   "source": [
    "## Create the dataset\n",
    "\n",
    "1. `load_imdb()`: Downloads the IMDB dataset once; each review is decoded from bytes and HTML entities are resolved.\n",
    "\n",
    "2. `basic_clean()`: Cleans the text by removing HTML tags, non-ASCII symbols, and collapsing multiple spaces into a single space. It also converts everything to lowercase and ignores empty lines.\n",
    "\n",
    "3. Shuffle the cleaned list so that train/validation splits are random.\n",
    "\n",
    "4. `compute_word_piece_vocabulary()`: Computes the WordPiece vocabulary from the cleaned text. It uses a greedy algorithm to merge the most frequent pairs of characters until it reaches a specified vocabulary size.\n",
    "\n",
    "5.  `WordPieceTokenizer(...)` uses the vocabulary map to convert text to fixed-length ID sequences. The extra + 1 token reserves room for the \"next token\" label.\n",
    "\n",
    "6. `make_ds()`: Creates a TensorFlow dataset from the tokenized text. It tokenizes each review, discards sequences shorter than two tokens, splits the data into input and target sequences, shuffles the dataset, pads the sequences to a uniform length, batches them, and prefetches them for efficient training.\n",
    " \n",
    "7. Train/val split: 90 % of the shuffled corpus feeds train_ds, the remaining 10% val_ds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9d3d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m             out\u001b[38;5;241m.\u001b[39mappend(ln)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 17\u001b[0m raw_text \u001b[38;5;241m=\u001b[39m basic_clean(\u001b[43mload_imdb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(raw_text)  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#raw_text   = raw_text[:200_000] # trim\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mload_imdb\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_imdb\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     raw \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain+test\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     text \u001b[38;5;241m=\u001b[39m [html\u001b[38;5;241m.\u001b[39munescape(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_imdb():\n",
    "    import tensorflow_datasets as tfds\n",
    "    raw = tfds.load(\"imdb_reviews\", split=\"train+test\", shuffle_files=True)\n",
    "    text = [html.unescape(x[\"text\"].numpy().decode()) for x in raw]\n",
    "    return text\n",
    "\n",
    "def basic_clean(lines):\n",
    "    out = []\n",
    "    pattern = re.compile(r\"<[^>]*>|[^A-Za-z0-9 ,.!?'\\n]\")\n",
    "    for ln in lines:\n",
    "        ln = pattern.sub(\" \", ln).lower()\n",
    "        ln = re.sub(r\"\\s+\", \" \", ln).strip()\n",
    "        if ln: \n",
    "            out.append(ln)\n",
    "    return out\n",
    "\n",
    "raw_text = basic_clean(load_imdb())\n",
    "random.shuffle(raw_text)  \n",
    "#raw_text   = raw_text[:200_000] # trim\n",
    "print(f\"Corpus: {len(raw_text):,} lines\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1.  Tokeniser  \n",
    "# ---------------------------------------------------------------------\n",
    "vocab_size = 8_000\n",
    "SEQ_LEN    = 256      \n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(raw_text)\n",
    "\n",
    "vocab = knlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    data=text_ds,\n",
    "    vocabulary_size=vocab_size,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "tokenizer = knlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN + 1,     \n",
    "    lowercase=True,\n",
    "    oov_token=\"[UNK]\",\n",
    ")\n",
    "\n",
    "def make_ds(texts, batch=64):     \n",
    "    toks = tokenizer(texts)\n",
    "    toks = [t[:SEQ_LEN+1] for t in toks if len(t) > 1]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(toks)\n",
    "\n",
    "    def xy(tokens):\n",
    "        return {\"tokens\": tokens[:-1]}, tokens[1:]\n",
    "\n",
    "    return (ds.map(xy, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .shuffle(50_000)\n",
    "              .padded_batch(batch, drop_remainder=True)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_split = int(0.9*len(raw_text))\n",
    "train_ds = make_ds(raw_text[:train_split])\n",
    "val_ds   = make_ds(raw_text[train_split:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b604f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie review    → ['movie', 'review']\n",
      "Transformer     → ['t', '##ran', '##s', '##form', '##er']\n",
      "xqzj            → ['x', '##q', '##z', '##j']\n",
      "abcdefghijklmnopqrstuvwxyz → ['abc', '##de', '##f', '##gh', '##i', '##j', '##k', '##lm', '##no', '##p', '##q', '##rst', '##u', '##v', '##w', '##x', '##y', '##z']\n"
     ]
    }
   ],
   "source": [
    "for txt in [\n",
    "    \"movie review\", \n",
    "    \"Transformer\", \n",
    "    \"xqzj\", \n",
    "    \"abcdefghijklmnopqrstuvwxyz\"]:\n",
    "\n",
    "    ids = tokenizer(txt)\n",
    "    subtokens = [tokenizer.vocabulary[i] \n",
    "                 for i in ids.numpy() if i != 0]\n",
    "    print(f\"{txt:15} → {subtokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2936b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7905\n",
      "Token IDs : tf.Tensor(\n",
      "[ 53  57  55 125   5   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0], shape=(257,), dtype=int32)\n",
      "Back to txt: this movie was great ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(vocab))                 # 8 000\n",
    "ids = tokenizer(\"This movie was great!\")\n",
    "print(\"Token IDs :\", ids)\n",
    "print(\"Back to txt:\", tokenizer.detokenize(ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555c18f",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "1. **Input tokens**  \n",
    "   * Each IMDB review is already tokenised into WordPiece IDs, giving an input tensor of shape `(batch, T)` with `T ≤ SEQ_LEN`.\n",
    "\n",
    "2. **Positional Embedding layer**  \n",
    "   * **Token embeddings** learn a `d_model`-dimensional vector for every vocabulary entry.  \n",
    "   * **Position embeddings** learn a `d_model`-dimensional vector for positions `0 … SEQ_LEN − 1`.  \n",
    "   * The layer **adds** the two vectors so the model receives both the token identity (*what*) and its location (*where*).\n",
    "\n",
    "3. **Causal mask**  \n",
    "   * A lower-triangular `(T, T)` mask ensures that, at time step *t*, the model attends only to positions `≤ t`.  \n",
    "   * This enforces the left-to-right, next-token-prediction objective.\n",
    "\n",
    "4. **`GPTBlock` &nbsp;— repeated `DEPTH = 8` times**  \n",
    "   Each block contains two residual sub-layers, both preceded by Layer Normalisation and followed by dropout `p = 0.1`.\n",
    "\n",
    "   | Sub-layer | Purpose | Key details |\n",
    "   |-----------|---------|-------------|\n",
    "   | **Multi-Head Self-Attention** | Lets each token look back at earlier tokens and weigh their relevance. | `HEADS = 8`, key/query size `d_model / HEADS = 32`, masked attention, dropout 0.1. |\n",
    "   | **Feed-Forward Network** | Refines each token representation independently. | Two dense layers: width expands to `4 × d_model` with **GELU** activation, then projects back to `d_model`; dropout inside and after. |\n",
    "\n",
    "5. **Language-model head**  \n",
    "   * A final dense layer of size `vocab_size` projects each `d_model` vector to logits over the vocabulary.  \n",
    "   * Training uses *Sparse Categorical Cross-Entropy* to predict the **next** token at every position.\n",
    "\n",
    "6. **Hyper-parameters**  \n",
    "   * `d_model = 256` keeps memory ≈ 6 GB (FP16).  \n",
    "   * `DEPTH = 8` gives enough depth without long runtimes.  \n",
    "   * `HEADS = 8` each head processes 32-dim keys & queries.  \n",
    "   * Dropout `p = 0.1` mitigates over-fitting on the small IMDB corpus.\n",
    "\n",
    "Together these pieces implement the core ideas behind GPT-style language models: position-aware token embeddings, masked self-attention for autoregression, and stacked attention/MLP blocks that build hierarchical representations while preserving gradient flow through residual connections.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f15771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, vocab, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.tok = keras.layers.Embedding(vocab, d_model)\n",
    "        self.pos = keras.layers.Embedding(max_len, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        idx = tf.range(tf.shape(x)[-1])[None]\n",
    "        return self.tok(x) + self.pos(idx)\n",
    "\n",
    "## A version for debugging and illustration\n",
    "class PositionalEmbeddingDebug(PositionalEmbedding):\n",
    "    def call(self, x, return_parts=False):\n",
    "        idx = tf.range(tf.shape(x)[-1])[None]\n",
    "        tok = self.tok(x)\n",
    "        pos = self.pos(idx)\n",
    "        return (tok, pos) if return_parts else tok + pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e9105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply PositionalEmbedding: \n",
      " tf.Tensor(\n",
      "[[[ 0.09493951 -0.09283178  0.01114397 -0.03076396]\n",
      "  [ 0.03453743 -0.00263811 -0.02565154 -0.01494424]\n",
      "  [ 0.0019802  -0.09410468  0.00568523  0.00466434]\n",
      "  [ 0.00854168  0.06907481 -0.00520728  0.04899843]\n",
      "  [-0.02489221 -0.01527636 -0.03607261 -0.09318761]]], shape=(1, 5, 4), dtype=float32)\n",
      "Token IDs:         [[1 4 1 3 0]]\n",
      "\n",
      "E_token (word vectors):\n",
      "[[[ 0.04694815 -0.04938451  0.03270758  0.00196652]\n",
      "  [-0.00088028  0.01174162 -0.03452749  0.01346072]\n",
      "  [ 0.04694815 -0.04938451  0.03270758  0.00196652]\n",
      "  [-0.03093035  0.0200041  -0.00651758  0.03676522]\n",
      "  [-0.0347316   0.00906425 -0.02840428 -0.04868952]]]\n",
      "\n",
      "E_pos (position vectors):\n",
      "[[[ 0.04799136 -0.04344727 -0.02156361 -0.03273048]\n",
      "  [ 0.03541771 -0.01437973  0.00887595 -0.02840496]\n",
      "  [-0.04496795 -0.04472017 -0.02702235  0.00269781]\n",
      "  [ 0.03947203  0.0490707   0.0013103   0.01223321]\n",
      "  [ 0.00983939 -0.02434061 -0.00766833 -0.04449809]]]\n",
      "\n",
      "Sum fed to Transformer:\n",
      "[[[ 0.09493951 -0.09283178  0.01114397 -0.03076396]\n",
      "  [ 0.03453743 -0.00263811 -0.02565154 -0.01494424]\n",
      "  [ 0.0019802  -0.09410468  0.00568523  0.00466434]\n",
      "  [ 0.00854168  0.06907481 -0.00520728  0.04899843]\n",
      "  [-0.02489221 -0.01527636 -0.03607261 -0.09318761]]]\n"
     ]
    }
   ],
   "source": [
    "# --- Toy demo wrapped in its own scope -------------------------------\n",
    "def show_positional_demo():\n",
    "\n",
    "    vocab_size, d_model, max_len = 6, 4, 10\n",
    "    layer  = PositionalEmbeddingDebug(vocab_size, d_model, max_len)\n",
    "\n",
    "    tokens = tf.constant([[1, 4, 1, 3, 0]])      # shape (batch=1, T=5)\n",
    "\n",
    "    print(\"Apply PositionalEmbedding: \\n\", layer(tokens))  \n",
    "\n",
    "    token_embs, pos_embs   = layer(tokens, return_parts=True)              # (1,5,4)\n",
    "\n",
    "    # 2) combined input\n",
    "    combined   = token_embs + pos_embs           # identical to layer(tokens)\n",
    "\n",
    "    # Nicely formatted printout\n",
    "    print(\"Token IDs:        \", tokens.numpy())\n",
    "    print(\"\\nE_token (word vectors):\")\n",
    "    print(token_embs.numpy())\n",
    "    print(\"\\nE_pos (position vectors):\")\n",
    "    print(pos_embs.numpy())\n",
    "    print(\"\\nSum fed to Transformer:\")\n",
    "    print(combined.numpy())\n",
    "\n",
    "show_positional_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229daa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mini_gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tokens (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_embedding            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,113,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTBlock</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tokens (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_embedding            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m2,113,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block (\u001b[38;5;33mGPTBlock\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_1 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_2 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_3 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_4 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_5 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_6 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gpt_block_7 (\u001b[38;5;33mGPTBlock\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m789,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m8000\u001b[0m)      │     \u001b[38;5;34m2,056,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,487,616</span> (40.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,487,616\u001b[0m (40.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,487,616</span> (40.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,487,616\u001b[0m (40.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "def causal_mask(n):\n",
    "    i = tf.range(n)[:, None]\n",
    "    j = tf.range(n)[None, :]\n",
    "    m = i >= j\n",
    "    return tf.cast(m, tf.int32)[None, None]\n",
    "\n",
    "\n",
    "class GPTBlock(layers.Layer):\n",
    "    def __init__(self, d_model, heads, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn  = layers.MultiHeadAttention(\n",
    "            num_heads=heads,\n",
    "            key_dim=d_model // heads,\n",
    "            dropout=p_drop       \n",
    "        )\n",
    "        self.drop1 = layers.Dropout(p_drop) \n",
    "\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.ff    = keras.Sequential([\n",
    "            layers.Dense(4*d_model, activation=\"gelu\"),\n",
    "            layers.Dropout(p_drop),        \n",
    "            layers.Dense(d_model),\n",
    "        ])\n",
    "        self.drop2 = layers.Dropout(p_drop)   \n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        mask = causal_mask(tf.shape(x)[1])\n",
    "\n",
    "        # attention branch\n",
    "        attn_out = self.attn(\n",
    "            self.norm1(x), \n",
    "            self.norm1(x),\n",
    "            attention_mask=mask,\n",
    "            training=training,\n",
    "        )\n",
    "        x = x + self.drop1(attn_out, training=training)\n",
    "\n",
    "        # feed-forward branch\n",
    "        ffn_out = self.ff(self.norm2(x), training=training)\n",
    "        x = x + self.drop2(ffn_out, training=training)\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  Model  \n",
    "# ---------------------------------------------------------------------\n",
    "d_model = 256          # keep width so RAM stays ~6 GB fp16\n",
    "DEPTH   = 8            \n",
    "HEADS   = 8            # ★ d_model // HEADS = 32/key\n",
    "\n",
    "inp = keras.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"tokens\")\n",
    "x   = PositionalEmbedding(vocab_size, d_model, SEQ_LEN)(inp)\n",
    "for _ in range(DEPTH):\n",
    "    x = GPTBlock(d_model, HEADS)(x)\n",
    "logits = keras.layers.Dense(vocab_size)(x)\n",
    "\n",
    "model = keras.Model(inp, logits, name=\"mini_gpt\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c126b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = causal_mask(10)   # shape (1, 1, 4, 4)\n",
    "mask[0, 0]               # drop the leading singleton dims for display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efeab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      " [[[ 1.266  0.526 -0.397 -0.039  0.399 -1.112]\n",
      "  [ 0.009 -0.185 -0.702  0.195  1.029 -0.787]\n",
      "  [ 0.544 -0.053 -0.347  0.102 -1.157  0.522]\n",
      "  [ 0.066  0.275 -0.756  0.473  0.309  0.013]]] \n",
      "\n",
      "LayerNorm x:\n",
      " [[[ 1.547  0.559 -0.673 -0.195  0.39  -1.628]\n",
      "  [ 0.136 -0.184 -1.035  0.442  1.816 -1.175]\n",
      "  [ 1.05   0.02  -0.487  0.288 -1.884  1.013]\n",
      "  [ 0.007  0.533 -2.062  1.031  0.618 -0.127]]] \n",
      "\n",
      "attn_out BEFORE residual dropout:\n",
      " [[[ 0.4   -0.294 -0.728 -2.582  1.524  1.285]\n",
      "  [ 0.383 -0.284 -0.788 -2.569  1.508  1.252]\n",
      "  [ 3.497  0.889 -1.177  3.68  -1.785  0.524]\n",
      "  [ 0.346 -0.161 -1.306 -2.412  1.388  1.15 ]]] \n",
      "\n",
      "attn_out AFTER residual dropout:\n",
      " [[[ 0.444 -0.327 -0.808 -2.868  1.693  1.428]\n",
      "  [ 0.425 -0.316 -0.875 -2.855  1.675  1.391]\n",
      "  [ 3.886  0.988 -1.308  4.089 -1.984  0.583]\n",
      "  [ 0.384 -0.179 -1.452 -2.68   1.542  1.277]]] \n",
      "\n",
      "Result y = x + dropout(attn_out):\n",
      " [[[ 1.71   0.199 -1.205 -2.907  2.092  0.316]\n",
      "  [ 0.434 -0.501 -1.577 -2.66   2.704  0.604]\n",
      "  [ 4.43   0.935 -1.655  4.191 -3.141  1.105]\n",
      "  [ 0.45   0.096 -2.208 -2.207  1.851  1.29 ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_gpt_layer_demo():\n",
    "    import numpy as np\n",
    "\n",
    "    #np.random.seed(0)\n",
    "\n",
    "    batch, T, d_model = 1, 4, 6          # 4-token sentence, 6-dim features\n",
    "    keep_prob_attn    = 0.9              # dropout inside attention weights\n",
    "    keep_prob_residual= 0.9              # dropout on attn_out\n",
    "    scale             = d_model ** -0.5  # 1/√d  for dot-product\n",
    "\n",
    "    # 1) fake input: token representations coming from previous layer\n",
    "    x = np.random.randn(batch, T, d_model).round(3)\n",
    "    print(\"Input x:\\n\", x, \"\\n\")\n",
    "\n",
    "    # 2) LayerNorm (per token)\n",
    "    mu  = x.mean(-1, keepdims=True)\n",
    "    var = x.var (-1, keepdims=True)\n",
    "    x_norm = (x - mu) / np.sqrt(var + 1e-5)\n",
    "    print(\"LayerNorm x:\\n\", x_norm.round(3), \"\\n\")\n",
    "\n",
    "    # 3) linear projections Q, K, V (random weights for the demo)\n",
    "    W_q, W_k, W_v = [np.random.randn(d_model, d_model) for _ in range(3)]\n",
    "    Q = x_norm @ W_q\n",
    "    K = x_norm @ W_k\n",
    "    V = x_norm @ W_v\n",
    "\n",
    "    # 4) causal mask: forbid looking right\n",
    "    mask = np.tril(np.ones((T, T), dtype=bool))   # (T,T) lower-triangle\n",
    "    scores = (Q @ K.transpose(0,2,1)) * scale     # (B,T,T)\n",
    "    scores[:, ~mask]  = -1e9                          # set future positions to −∞\n",
    "\n",
    "    # 5) soft-max to get attention weights\n",
    "    def softmax(a, axis=-1):\n",
    "        a_exp = np.exp(a - a.max(axis=axis, keepdims=True))\n",
    "        return a_exp / a_exp.sum(axis=axis, keepdims=True)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "\n",
    "    # 6) **Attention-weight dropout** (drop some links)\n",
    "    drop_mask_attn = (np.random.rand(*weights.shape) < keep_prob_attn)\n",
    "    weights_drop   = weights * drop_mask_attn / keep_prob_attn   # rescale to keep expectation\n",
    "\n",
    "    # 7) weighted sum → attn_out\n",
    "    attn_out = weights_drop @ V\n",
    "    print(\"attn_out BEFORE residual dropout:\\n\", attn_out.round(3), \"\\n\")\n",
    "\n",
    "    # 8) **Residual-dropout** on attn_out\n",
    "    drop_mask_res = (np.random.rand(*attn_out.shape) < keep_prob_residual)\n",
    "    attn_out_rd   = attn_out * drop_mask_res / keep_prob_residual\n",
    "\n",
    "    # 9) residual add\n",
    "    y = x + attn_out_rd\n",
    "    print(\"attn_out AFTER residual dropout:\\n\", attn_out_rd.round(3), \"\\n\")\n",
    "    print(\"Result y = x + dropout(attn_out):\\n\", y.round(3))\n",
    "\n",
    "show_gpt_layer_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a96eff",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "We use the AdamW optimizer with weight decay, a learning rate of 1e-4, and a batch size of 64. The model is trained for 10 epochs with a learning rate schedule that decays the learning rate by 0.1 every 3 epochs. \n",
    "\n",
    "The training loss is monitored using the sparse categorical cross-entropy loss function:  \n",
    "\n",
    "| Position *t* | True next token | Model’s soft-max probability | Token-level loss |\n",
    "|--------------|-----------------|-----------------------------|------------------|\n",
    "| 0 | `\"I\"`     | **0.40** | \\(-\\log 0.40 \\approx 0.92\\) |\n",
    "| 1 | `\"love\"`  | **0.05** | \\(-\\log 0.05 \\approx 2.99\\) |\n",
    "| 2 | `\"this\"`  | **0.60** | \\(-\\log 0.60 \\approx 0.51\\) |\n",
    "| 3 | `\"movie\"` | **0.10** | \\(-\\log 0.10 \\approx 2.30\\) |\n",
    "\n",
    "**Average loss**\n",
    "\n",
    "$$\n",
    "\\frac{0.92 + 2.99 + 0.51 + 2.30}{4} \\;\\approx\\; 1.68\n",
    "$$\n",
    "\n",
    "Keras reports this *1.68* during training; the optimiser tries to push it lower by assigning higher probability to the correct next token at each position.\n",
    "\n",
    "The model is saved after each epoch, and the best model is selected based on the validation loss.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697f2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine-decay restart LR schedule (two-epoch cycles)\n",
    "steps_per_epoch = len(train_ds)\n",
    "lr_sched = keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate=3e-4,\n",
    "    first_decay_steps=steps_per_epoch*2,\n",
    ")\n",
    "opt = keras.optimizers.AdamW(lr_sched, weight_decay=1e-4)\n",
    "\n",
    "loss  = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(opt, loss, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcde78",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5342704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: tokens\n",
      "Received: inputs=['Tensor(shape=(64, 256))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3690s\u001b[0m 5s/step - accuracy: 0.2922 - loss: 4.7974 - val_accuracy: 0.3369 - val_loss: 4.0557\n",
      "Epoch 2/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3656s\u001b[0m 5s/step - accuracy: 0.3345 - loss: 4.0513 - val_accuracy: 0.3444 - val_loss: 3.9677\n",
      "Epoch 3/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3659s\u001b[0m 5s/step - accuracy: 0.3415 - loss: 3.9703 - val_accuracy: 0.3625 - val_loss: 3.7509\n",
      "Epoch 4/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3704s\u001b[0m 5s/step - accuracy: 0.3575 - loss: 3.7548 - val_accuracy: 0.3740 - val_loss: 3.6263\n",
      "Epoch 5/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3647s\u001b[0m 5s/step - accuracy: 0.3683 - loss: 3.6355 - val_accuracy: 0.3796 - val_loss: 3.5723\n",
      "Epoch 6/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3644s\u001b[0m 5s/step - accuracy: 0.3735 - loss: 3.5783 - val_accuracy: 0.3809 - val_loss: 3.5615\n",
      "Epoch 7/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3557s\u001b[0m 5s/step - accuracy: 0.3708 - loss: 3.6077 - val_accuracy: 0.3848 - val_loss: 3.5232\n",
      "Epoch 8/8\n",
      "\u001b[1m703/703\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3551s\u001b[0m 5s/step - accuracy: 0.3792 - loss: 3.5232 - val_accuracy: 0.3918 - val_loss: 3.4593\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=2,\n",
    "                                      restore_best_weights=True),\n",
    "        keras.callbacks.ModelCheckpoint(\"mini_gpt.keras\",\n",
    "                                        save_best_only=True),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3519b",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d30771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD id : 0 UNK id : 3 vocab : 7905\n"
     ]
    }
   ],
   "source": [
    "PAD  = tokenizer.token_to_id(\"[PAD]\")\n",
    "UNK  = tokenizer.token_to_id(\"[UNK]\")          # handy for debugging\n",
    "MASK = tokenizer.token_to_id(\"[MASK]\")         # not used here but nice to have\n",
    "vocab_size = tokenizer.vocabulary_size()\n",
    "\n",
    "print(\"PAD id :\", PAD,  \"UNK id :\", UNK, \"vocab :\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63322dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was a wonderful mixture of characters, and most of the actors really\n",
      "were believable. The script is not bad but a great film. I would say that for\n",
      "another person involved should make this movie. Well then we should see why the\n",
      "actors are such a truly bad movie. But because I'd put the soundtrack by someone\n",
      "who actually made it very believable. I think it's a little too. [PAD]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  Constants  (define once, reuse everywhere)\n",
    "# ------------------------------------------------------------------\n",
    "# constants (already defined)\n",
    "SEQ_LEN    = 256\n",
    "VOCAB_DIM  = 8000\n",
    "REAL_VOCAB = tokenizer.vocabulary_size()\n",
    "PAD_ID     = tokenizer.token_to_id(\"[PAD]\")\n",
    "UNK_ID     = tokenizer.token_to_id(\"[UNK]\")\n",
    "\n",
    "def top_k_logits(logits, k=40):\n",
    "    vals, _   = tf.math.top_k(logits, k=k)\n",
    "    min_vals  = vals[..., -1, None]\n",
    "    return tf.where(logits < min_vals, tf.float32.min, logits)\n",
    "\n",
    "import re, textwrap\n",
    "\n",
    "def tidy(text, width = 80):\n",
    "    \"\"\"Fix spacing, punctuation, sentence caps, and wrap to `width` columns.\"\"\"\n",
    "    \n",
    "    # remove space before punctuation  –>  \"word , ...\"  → \"word, ...\"\n",
    "    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "\n",
    "    # ensure single space after punctuation\n",
    "    text = re.sub(r\"([.,!?;:])([^\\s])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # fix contractions\n",
    "    text = re.sub(r\"\\b(\\w+)\\s+'\\s*([sSdDmMnt]|re|ve|ll)\\b\", r\"\\1'\\2\", text)\n",
    "\n",
    "    # capitalise \" i \" → \" I \"  (pronoun) and sentence starts\n",
    "    def cap_sentence(m):\n",
    "        return m.group(1) + m.group(2).upper()\n",
    "    text = re.sub(r\"(^|[.!?]\\s+)([a-z])\", cap_sentence, text)\n",
    "    text = re.sub(r\"\\bi\\b\", \"I\", text)\n",
    "\n",
    "    # collapse multiple spaces, strip ends\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "    # wrap into neat paragraphs\n",
    "    return textwrap.fill(text, width)\n",
    "\n",
    "def sample(prompt, max_new=80, temperature=1.0, k=40):\n",
    "    ids = tokenizer(prompt).numpy().tolist()\n",
    "    ids = ids if isinstance(ids[0], int) else ids[0]\n",
    "\n",
    "    while ids and ids[-1] == PAD_ID:\n",
    "        ids.pop()\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        ctx = ids[-SEQ_LEN:]\n",
    "        x   = np.array(ctx + [PAD_ID]*(SEQ_LEN-len(ctx)))[None]\n",
    "\n",
    "        logits  = model.predict(x, verbose=0)[0, len(ctx)-1]\n",
    "        if temperature == 0.0:                 # ★ greedy decode\n",
    "            next_id = int(np.argmax(logits))\n",
    "        else:                                  # ★ stochastic decode\n",
    "            logits  = top_k_logits(logits / temperature, k).numpy()\n",
    "            next_id = np.random.choice(VOCAB_DIM, p=tf.nn.softmax(logits).numpy())\n",
    "\n",
    "        if next_id >= REAL_VOCAB:\n",
    "            next_id = UNK_ID\n",
    "\n",
    "        ids.append(next_id)\n",
    "        if next_id == PAD_ID:\n",
    "            break\n",
    "\n",
    "    return tidy(tokenizer.detokenize([ids])[0].strip())\n",
    "\n",
    "print(sample(\"the movie was\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0f577a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was great. If this movie was produced, a copy of the original vhs\n",
      "version, the dvd will appeal to your dvd collection and it. It may be very\n",
      "effective, but as I can say, the whole thing at the end was that it was so real\n",
      "for me was too much to find. .. But a real piece of work of view. .. And if you\n",
      "are willing to enjoy the whole movie to go further then this should be done\n",
      "better. It would have been true to watch the original, it will be true to the\n",
      "time and so much more accurate that that the movie has been made by a scummer.\n",
      "This movie is a shame for the director. It has my heart that shows an excellent\n",
      "book of the movie. . So I could not recommend this film to everyone, but I wish\n",
      "to watch it. [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(sample(\"the movie was \", max_new=200, temperature=1.0, k=40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a815a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}