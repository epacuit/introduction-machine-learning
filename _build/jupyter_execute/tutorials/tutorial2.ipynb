{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d92a650b80b6df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/epacuit/introduction-machine-learning/blob/main/tutorials/tutorial2.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "# Tutorial 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43370a8883159cd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbb4d9dfa0621d4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1. Reading `csv` files\n",
    "\n",
    "A `csv` file is a comma separated values file. It is a simple file format used to store tabular data, such as a spreadsheet or database. The first row of the file typically contains the column names, and the following rows contain the data.\n",
    "\n",
    "The file `comedy_comparisons_metadata.csv` contains metadata about videos on YouTube.  The file is available at the following URL: [https://raw.githubusercontent.com/epacuit/introduction-machine-learning/refs/heads/main/tutorials/comedy_comparisons_metadata.csv](https://raw.githubusercontent.com/epacuit/introduction-machine-learning/refs/heads/main/tutorials/comedy_comparisons_metadata.csv)\n",
    "\n",
    "Use the `csv` Python package ([https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)) to read the file.   Create a list `metadata` that contains dictionaries for each row in the file.  The keys of the dictionaries should be the column names and the values should be the corresponding values in the row.  For example, the first dictionary in the list should be: \"video_id\", \"title\", \"view_count\", \"like_count\", \"comment_count\", \"duration\", corresponding to the columns in the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b03224fdab26646",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "metadata = []\n",
    "with open('./comedy_comparisons_metadata.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header= next(reader) \n",
    "    for row in reader:\n",
    "        metadata.append({\n",
    "            \"video_id\": row[0],\n",
    "            \"title\": row[1],\n",
    "            \"view_count\": row[2],\n",
    "            \"like_count\": row[3],\n",
    "            \"comment_count\": row[4],\n",
    "            \"duration\": row[5],\n",
    "        })\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-acf802fd0f3de227",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(metadata) == 11541\n",
    "assert all([type(x) == dict for x in metadata])\n",
    "assert all([type(x) == dict for x in metadata])\n",
    "assert all([sorted(list(x.keys())) == sorted(['video_id', 'duration', 'title', 'view_count', 'like_count', 'comment_count', ]) for x in metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48d6cbc786b09b8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def avg_view_count(metadata):\n",
    "    \"\"\"\n",
    "    Calculate the average view count of the videos in the metadata.   Return the average rounded to two decimal places.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    total = 0\n",
    "    for video in metadata:\n",
    "        total += int(video['view_count']) if video['view_count'] else 0\n",
    "    return round(total / len(metadata), 2)\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a429161ba39deee2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert avg_view_count(metadata) == 891988.54\n",
    "assert avg_view_count(metadata[100:200]) == 1152895.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-653621e2550c1bb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. Write a function that accepts the `metadata` list, the `video_id`, and a column name, and return the value of the column name for the video_id. \n",
    "\n",
    "For instance, `get_value(metadata, 'DE1-cD3pTkA', 'like_count')` should the number of likes for the video with id \"DE1-cD3pTkA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccb737d604c12008",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_value(metadata, video_id, col_name):\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    num =  [md for md in metadata if md['video_id'] == video_id][0].get(col_name, 0)  \n",
    "\n",
    "    return int(num) if num != '' else 0\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-50c4e72489e49f4c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_value(metadata, 'XZqSz_X-j8Y', 'view_count') == 1919\n",
    "assert get_value(metadata, 'XZqSz_X-j8Y', 'like_count') == 7\n",
    "assert get_value(metadata, 'XZqSz_X-j8Y', 'comment_count') == 3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3580fd0d7c34dea5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 2: Predicting Video Comparisons from Metadata\n",
    "\n",
    "In this part, we will attempt to predict which of two YouTube videos is considered funnier based on their metadata.  \n",
    "\n",
    "The dataset `comedy_comparisons.csv` is a subset of the *YouTube Comedy Slam Preference* dataset, available from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/223/youtube+comedy+slam+preference+data). It contains pairwise comparisons of videos, where each row records the video IDs of two videos and indicates which one was rated as funnier by a user.  \n",
    "\n",
    "You can access the file at the following URL:  \n",
    "[https://raw.githubusercontent.com/epacuit/introduction-machine-learning/refs/heads/main/tutorials/test_comedy_comparisons_restricted.csv](https://raw.githubusercontent.com/epacuit/introduction-machine-learning/refs/heads/main/tutorials/test_comedy_comparisons_restricted.csv).\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Read the Dataset**: Read the file `test_comedy_comparisons_restricted.csv` and create a list of dictionaries. Each dictionary should have the keys `\"video_id_1\"`, `\"video_id_2\"`, and `\"winner\"`.  \n",
    "   - `\"video_id_1\"` and `\"video_id_2\"` should store the video IDs being compared.  \n",
    "   - `\"winner\"` should be `1` if `video_id_1` is considered funnier and `0` if `video_id_2` is considered funnier.  \n",
    "   - The function should return a list of such dictionaries.  \n",
    "\n",
    "2. **Implement Comparison Functions**: Write three different comparison functions of the following form:  \n",
    "\n",
    "   ```python\n",
    "   def is_funnier(video_id_1, video_id_2, metadata):\n",
    "       \"\"\"\n",
    "       Returns True if video_id_1 is predicted to be funnier than video_id_2 based on metadata.\n",
    "       \"\"\"\n",
    "    ```\n",
    "\n",
    "    Each function should predict which video is funnier based on some metadata attribute, such as:\n",
    "        - Number of views\n",
    "        - Number of likes\n",
    "        - Number of comments\n",
    "\n",
    "3. **Evaluate Accuracy**: Write a function `evaluate` that accepts the list of comparisons created in step 1 and evaluates the *accuracy* of a  comparison function.  The accuracy is the proportion of comparisons where the function correctly predicts the funnier video.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a9f20d1732fa1350",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "comparisons = []\n",
    "\n",
    "## read the file test_comedy_comparisons_restricted.csv and store the dictionary in the list comparisons\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eab5374ddf430e4b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_funnier_1(metadata, video_id1, video_id2):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a8f8759cc837625a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def is_funnier_2(metadata, video_id1, video_id2):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0dbd1b91fa7add6e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def is_funnier_3(metadata, video_id1, video_id2):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56dd635773e69030",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(metadata, comparisons, is_funnier):\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be5855b232f9f628",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe accuracy of is_funnier_1 is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomparisons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_funnier_1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe accuracy of is_funnier_2 is\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluate(metadata, comparisons, is_funnier_2))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe accuracy of is_funnier_3 is\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluate(metadata, comparisons, is_funnier_3))\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(metadata, comparisons, is_funnier)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(metadata, comparisons, is_funnier):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m### BEGIN SOLUTION\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The accuracy of is_funnier_1 is\", evaluate(metadata, comparisons, is_funnier_1))\n",
    "\n",
    "print(\"The accuracy of is_funnier_2 is\", evaluate(metadata, comparisons, is_funnier_2))\n",
    "\n",
    "print(\"The accuracy of is_funnier_3 is\", evaluate(metadata, comparisons, is_funnier_3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}